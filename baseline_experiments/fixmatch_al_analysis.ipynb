{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7516aa",
   "metadata": {},
   "source": [
    "## FixMatch active learning analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63596358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86aa77c4",
   "metadata": {},
   "source": [
    "## FixMatch Baseline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd26656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "\n",
    "# Global font settings\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14,\n",
    "    \"font.weight\": \"bold\",\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"xtick.labelsize\": 13,\n",
    "    \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 13,\n",
    "    \"legend.frameon\": False\n",
    "})\n",
    "\n",
    "# define strategy as global variable\n",
    "strategy = \"\"\n",
    "\n",
    "\n",
    "# === Classifier Definition ===\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), nn.ReLU(),\n",
    "            nn.Linear(512, 384), nn.ReLU(),\n",
    "            nn.Linear(384, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 100), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(100, 100), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(100, num_classes)\n",
    "            #nn.Linear(100, 1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.encoder(x))\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input features to a lower-dimensional representation.\"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer,\n",
    "                                    num_warmup_steps,\n",
    "                                    num_training_steps,\n",
    "                                    num_cycles=7./16.,\n",
    "                                    last_epoch=-1):\n",
    "    def _lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        no_progress = float(current_step - num_warmup_steps) / \\\n",
    "            float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0., math.cos(math.pi * num_cycles * no_progress))\n",
    "\n",
    "    return LambdaLR(optimizer, _lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "\n",
    "def split_labeled_unlabeled(X, y, labeled_ratio=0.1, stratify=True, random_state=42):\n",
    "    n_samples = len(X)\n",
    "    n_labeled = int(n_samples * labeled_ratio)\n",
    "    if stratify:\n",
    "        X_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(\n",
    "            X, y, train_size=n_labeled, stratify=y, random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        X_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(\n",
    "            X, y, train_size=n_labeled, random_state=random_state\n",
    "        )\n",
    "    return X_labeled, y_labeled, X_unlabeled, y_unlabeled\n",
    "\n",
    "def random_bit_flip(x, n_bits=1):\n",
    "    x_aug = x.clone()\n",
    "    batch_size, num_features = x.shape\n",
    "    for i in range(batch_size):\n",
    "        flip_indices = torch.randperm(num_features)[:n_bits]\n",
    "        x_aug[i, flip_indices] = 1 - x_aug[i, flip_indices]\n",
    "    return x_aug\n",
    "\n",
    "def random_bit_flip_bernoulli(x, p=None, n_bits=None):\n",
    "    \"\"\"\n",
    "    Randomly flip each bit in the input tensor with probability p using Bernoulli distribution.\n",
    "    If n_bits is given, p is set so that on average n_bits are flipped per sample.\n",
    "    Args:\n",
    "        x: Tensor of shape (batch_size, num_features)\n",
    "        p: Probability of flipping each bit (float, between 0 and 1)\n",
    "        n_bits: If given, overrides p so that p = n_bits / num_features\n",
    "    Returns:\n",
    "        Augmented tensor with bits flipped\n",
    "    \"\"\"\n",
    "    x_aug = x.clone()\n",
    "    batch_size, num_features = x.shape\n",
    "    device = x.device\n",
    "    # if n_bits is not None:\n",
    "    #     p = float(n_bits * 10) / num_features\n",
    "    # else:\n",
    "    #     if p is None:\n",
    "    #         p = 0.01  # default\n",
    "    #     else:\n",
    "    #         p = float(p)\n",
    "    if p is not None:\n",
    "        p = float(p)\n",
    "    else:\n",
    "        p = 0.01  \n",
    "    flip_mask = torch.bernoulli(torch.full_like(x_aug, p, device=device))\n",
    "    x_aug = torch.abs(x_aug - flip_mask)\n",
    "    return x_aug\n",
    "\n",
    "def random_feature_mask(x, n_mask=1):\n",
    "    x_aug = x.clone()\n",
    "    batch_size, num_features = x.shape\n",
    "    for i in range(batch_size):\n",
    "        mask_indices = torch.randperm(num_features)[:n_mask]\n",
    "        x_aug[i, mask_indices] = 0\n",
    "    return x_aug\n",
    "\n",
    "def random_bit_flip_and_mask(x, n_bits=1, n_mask=1):\n",
    "    x_aug = random_bit_flip(x, n_bits=n_bits)\n",
    "    x_aug = random_feature_mask(x_aug, n_mask=n_mask)\n",
    "    return x_aug\n",
    "\n",
    "\n",
    "def train_fixmatch_drift_eval(\n",
    "    bit_flip, model, optimizer, X_labeled, y_labeled, X_unlabeled, y_unlabeled,\n",
    "    args, num_classes=2, threshold=0.95, lambda_u=1.0, epochs=200, batch_size=64\n",
    "):\n",
    "    labeled_ds = TensorDataset(X_labeled, y_labeled)\n",
    "    unlabeled_ds = TensorDataset(X_unlabeled)\n",
    "\n",
    "    train_sampler = RandomSampler if args.local_rank == -1 else DistributedSampler\n",
    "\n",
    "\n",
    "    labeled_loader = DataLoader(labeled_ds, sampler=train_sampler(labeled_ds), batch_size=batch_size, drop_last=True)\n",
    "    unlabeled_loader = DataLoader(unlabeled_ds, sampler=train_sampler(unlabeled_ds), batch_size=batch_size, drop_last=True)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    \n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, args.warmup, epochs)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_state_dict = None\n",
    "\n",
    "    mu = 1  # Number of unlabeled augmentations per sample (FixMatch default is 1)\n",
    "    interleave_size = 2 * mu + 1  # labeled, unlabeled_weak, unlabeled_strong\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        labeled_iter = iter(labeled_loader)\n",
    "        unlabeled_iter = iter(unlabeled_loader)\n",
    "\n",
    "        for _ in range(len(labeled_loader)):\n",
    "            try:\n",
    "                x_l, y_l = next(labeled_iter)\n",
    "                (x_u,) = next(unlabeled_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            x_l, y_l = x_l.cuda(), y_l.cuda()\n",
    "            x_u = x_u.cuda()\n",
    "            p = 0.0\n",
    "            # Weak and strong augmentations for unlabeled data, now with seed\n",
    "            if args.aug == \"random_bit_flip\":\n",
    "                x_u_w = random_bit_flip(x_u, n_bits=1)\n",
    "                x_u_s = random_bit_flip(x_u, n_bits=bit_flip)\n",
    "            elif args.aug == \"random_bit_flip_bernoulli\":\n",
    "                p = 0.05\n",
    "                x_u_w = random_bit_flip_bernoulli(x_u, p=0.01, n_bits=None)\n",
    "                x_u_s = random_bit_flip_bernoulli(x_u, p=p, n_bits=None)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown augmentation function: {args.aug}\")\n",
    "\n",
    "            # Interleave all inputs for batchnorm consistency\n",
    "            inputs = torch.cat([x_l, x_u_w, x_u_s], dim=0)\n",
    "\n",
    "            logits = model(inputs)\n",
    "\n",
    "            batch_size = x_l.shape[0]\n",
    "            logits_x = logits[:batch_size]\n",
    "            logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
    "\n",
    "            # Labeled loss\n",
    "            loss_x = criterion(logits_x, y_l)\n",
    "\n",
    "            # Unlabeled loss (FixMatch pseudo-labeling)\n",
    "            with torch.no_grad():\n",
    "                pseudo_logits = F.softmax(logits_u_w / args.T, dim=1)\n",
    "                pseudo_labels = torch.argmax(pseudo_logits, dim=1)\n",
    "                max_probs, _ = torch.max(pseudo_logits, dim=1)\n",
    "                mask = max_probs.ge(threshold).float()\n",
    "\n",
    "            loss_u = (F.cross_entropy(logits_u_s, pseudo_labels, reduction='none') * mask).mean()\n",
    "            loss = loss_x + lambda_u * loss_u\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            best_state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}: loss={total_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "    # Restore best model after initial training\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "\n",
    "    # === Evaluate on each year's test set ===\n",
    "    path = \"/home/mhaque3/myDir/data/gen_apigraph_drebin/\"\n",
    "    metrics_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for year in range(2013, 2019):\n",
    "            for month in range(1, 13):\n",
    "                try:\n",
    "                    data = np.load(f\"{path}{year}-{month:02d}_selected.npz\")\n",
    "                    X_raw = data[\"X_train\"]\n",
    "                    y_true = (data[\"y_train\"] > 0).astype(int)\n",
    "                    X_test = torch.tensor(X_raw, dtype=torch.float32).cuda()\n",
    "                    y_test = torch.tensor(y_true, dtype=torch.long).cuda()\n",
    "                    plot_malware_tsne_with_boundary(\n",
    "                        model,\n",
    "                        X_labeled, y_labeled,\n",
    "                        X_unlabeled, y_unlabeled,\n",
    "                        X_test, y_test,\n",
    "                        strategy=strategy + f\"_{year}_{month}\",\n",
    "                        boundary_k=200\n",
    "                    )\n",
    "\n",
    "                    logits = model(X_test)\n",
    "                    probs = torch.softmax(logits, dim=1) if logits.shape[1] > 1 else torch.sigmoid(logits)\n",
    "                    preds = logits.argmax(dim=1)\n",
    "                    y_true = y_test.cpu().numpy()\n",
    "                    y_pred = preds.cpu().numpy()\n",
    "                    if probs.shape[1] == 2:\n",
    "                        y_score = probs[:, 1].cpu().numpy()\n",
    "                    else:\n",
    "                        y_score = probs.cpu().numpy()  # for multi-class\n",
    "\n",
    "                    acc = accuracy_score(y_true, y_pred)\n",
    "                    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "                    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "                    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "                    cm = confusion_matrix(y_true, y_pred)\n",
    "                    if cm.shape == (2, 2):\n",
    "                        tn, fp, fn, tp = cm.ravel()\n",
    "                        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "                        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "                    else:\n",
    "                        fnr = fpr = float('nan')\n",
    "\n",
    "                    # ROC-AUC and PR-AUC (binary or multiclass)\n",
    "                    try:\n",
    "                        if probs.shape[1] == 2:\n",
    "                            roc_auc = roc_auc_score(y_true, y_score)\n",
    "                            pr_auc = average_precision_score(y_true, y_score)\n",
    "                        else:\n",
    "                            roc_auc = roc_auc_score(y_true, probs.cpu().numpy(), multi_class='ovr')\n",
    "                            pr_auc = average_precision_score(y_true, probs.cpu().numpy(), average='weighted')\n",
    "                    except Exception:\n",
    "                        roc_auc = pr_auc = float('nan')\n",
    "\n",
    "                    metrics_list.append({\n",
    "                        'year': f\"{year}_{month}\",\n",
    "                        'accuracy': acc,\n",
    "                        'precision': prec,\n",
    "                        'recall': rec,\n",
    "                        'f1': f1,\n",
    "                        'fnr': fnr,\n",
    "                        'fpr': fpr,\n",
    "                        'roc_auc': roc_auc,\n",
    "                        'pr_auc': pr_auc\n",
    "                    })\n",
    "\n",
    "                    print(f\"Year {year}_{month}: Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}, FNR={fnr:.4f}, FPR={fpr:.4f}, ROC-AUC={roc_auc:.4f}, PR-AUC={pr_auc:.4f}\")\n",
    "\n",
    "                except FileNotFoundError:\n",
    "                    continue\n",
    "\n",
    "    # Save results to CSV\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    metrics_df.to_csv(f\"analysis/{strategy}.csv\", index=False)\n",
    "\n",
    "    print(f\"Mean F1 Scores: {metrics_df['f1'].mean():.4f}\")\n",
    "    print(f\"Mean False Negative Rates: {metrics_df['fnr'].mean()}\")\n",
    "    print(f\"Mean False Positive Rates: {metrics_df['fpr'].mean()}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba867da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_malware_tsne_with_boundary(model, X_labeled, y_labeled, X_unlabeled, y_unlabeled, X_test, y_test, strategy, boundary_k=200):\n",
    "    \"\"\"\n",
    "    Plots t-SNE visualization of labeled, unlabeled, and test data with decision boundary samples highlighted.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained model with an encode() method.\n",
    "        X_labeled, y_labeled: Tensor - labeled data and labels\n",
    "        X_unlabeled, y_unlabeled: Tensor - unlabeled data and (true or pseudo) labels\n",
    "        X_test, y_test: Tensor - test data and true labels\n",
    "        boundary_k: int - number of boundary samples to highlight\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode all sets using the model\n",
    "        X_labeled_enc = model.encode(X_labeled).cpu()\n",
    "        X_unlabeled_enc = model.encode(X_unlabeled).cpu()\n",
    "        X_test_enc = model.encode(X_test).cpu()\n",
    "\n",
    "    # Stack for t-SNE\n",
    "    X_all = torch.cat([X_labeled_enc, X_unlabeled_enc, X_test_enc], dim=0)\n",
    "\n",
    "    # Apply t-SNE\n",
    "    X_2d = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X_all)\n",
    "\n",
    "    # Split into individual groups\n",
    "    n_lab = X_labeled.shape[0]\n",
    "    n_unlab = X_unlabeled.shape[0]\n",
    "    X_lab_2d = X_2d[:n_lab]\n",
    "    X_unlab_2d = X_2d[n_lab:n_lab + n_unlab]\n",
    "    X_test_2d = X_2d[n_lab + n_unlab:]\n",
    "\n",
    "    y_lab = y_labeled.cpu()\n",
    "    y_unlab = y_unlabeled.cpu()\n",
    "    y_tst = y_test.cpu()\n",
    "\n",
    "    # ---------- Find boundary samples in X_test ----------\n",
    "    with torch.no_grad():\n",
    "        logits_test = model(X_test)\n",
    "        probs_test = F.softmax(logits_test, dim=1)\n",
    "        top2 = torch.topk(probs_test, 2, dim=1).values\n",
    "        margins = top2[:, 0] - top2[:, 1]\n",
    "        boundary_indices = torch.argsort(margins)[:boundary_k]\n",
    "        # Using Prediction Confidence:\n",
    "        # import torch.nn.functional as F\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     logits = model(X_test)\n",
    "        #     probs = F.softmax(logits, dim=1)\n",
    "        #     confidence = probs.max(dim=1).values  # shape: (N,)\n",
    "        # boundary_indices = torch.argsort(confidence)[:top_k]\n",
    "\n",
    "\n",
    "\n",
    "    X_boundary_2d = X_test_2d[boundary_indices.cpu()]\n",
    "\n",
    "    # ---------- Plot ----------\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Labeled\n",
    "    plt.scatter(X_lab_2d[y_lab == 0, 0], X_lab_2d[y_lab == 0, 1], c='green', label='Labeled Benign', alpha=0.6, marker='o')\n",
    "    plt.scatter(X_lab_2d[y_lab == 1, 0], X_lab_2d[y_lab == 1, 1], c='red', label='Labeled Malware', alpha=0.6, marker='o')\n",
    "\n",
    "    # Unlabeled\n",
    "    plt.scatter(X_unlab_2d[y_unlab == 0, 0], X_unlab_2d[y_unlab == 0, 1], c='lightgreen', label='Unlabeled Benign', alpha=0.4, marker='s')\n",
    "    plt.scatter(X_unlab_2d[y_unlab == 1, 0], X_unlab_2d[y_unlab == 1, 1], c='salmon', label='Unlabeled Malware', alpha=0.4, marker='s')\n",
    "\n",
    "    # Test\n",
    "    plt.scatter(X_test_2d[y_tst == 0, 0], X_test_2d[y_tst == 0, 1], c='blue', label='Test Benign', alpha=0.6, marker='^')\n",
    "    plt.scatter(X_test_2d[y_tst == 1, 0], X_test_2d[y_tst == 1, 1], c='purple', label='Test Malware', alpha=0.6, marker='^')\n",
    "\n",
    "    # Boundary samples\n",
    "    plt.scatter(X_boundary_2d[:, 0], X_boundary_2d[:, 1], facecolors='none', edgecolors='black',\n",
    "                linewidths=1.5, s=100, label=f'Boundary Samples (top {boundary_k})')\n",
    "\n",
    "    plt.title(\"t-SNE of Malware Dataset with Decision Boundary Samples\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"analysis/{strategy}_tsne_boundary.png\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f4352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fixmatch_wo_al_random_bit_flip_11_lbr_0.4_seed_0...\n",
      "Using 11 bits to flip per sample. Labeled ratio: 0.4, Seed: 0\n",
      "Epoch 1: loss=65.2853\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m grouped_parameters \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     69\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: [p \u001b[38;5;28;01mfor\u001b[39;00m n, p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m     70\u001b[0m         nd \u001b[38;5;129;01min\u001b[39;00m n \u001b[38;5;28;01mfor\u001b[39;00m nd \u001b[38;5;129;01min\u001b[39;00m no_decay)], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m: args\u001b[38;5;241m.\u001b[39mwdecay},\n\u001b[1;32m     71\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: [p \u001b[38;5;28;01mfor\u001b[39;00m n, p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m     72\u001b[0m         nd \u001b[38;5;129;01min\u001b[39;00m n \u001b[38;5;28;01mfor\u001b[39;00m nd \u001b[38;5;129;01min\u001b[39;00m no_decay)], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m}\n\u001b[1;32m     73\u001b[0m ]\n\u001b[1;32m     74\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(grouped_parameters, lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr,\n\u001b[1;32m     75\u001b[0m                         momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, nesterov\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnesterov)\n\u001b[0;32m---> 77\u001b[0m \u001b[43mtrain_fixmatch_drift_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_bit_flip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_2012_labeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_2012_labeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_2012_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mtrain_fixmatch_drift_eval\u001b[0;34m(bit_flip, model, optimizer, X_labeled, y_labeled, X_unlabeled, args, num_classes, threshold, lambda_u, epochs, batch_size)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Weak and strong augmentations for unlabeled data, now with seed\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39maug \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_bit_flip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 185\u001b[0m     x_u_w \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_bit_flip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     x_u_s \u001b[38;5;241m=\u001b[39m random_bit_flip(x_u, n_bits\u001b[38;5;241m=\u001b[39mbit_flip)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39maug \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_bit_flip_bernoulli\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mrandom_bit_flip\u001b[0;34m(x, n_bits)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m     96\u001b[0m     flip_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(num_features)[:n_bits]\n\u001b[0;32m---> 97\u001b[0m     x_aug[i, flip_indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m x_aug[i, flip_indices]\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_aug\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "args_list = [\n",
    "    \"--bit_flip\", \"11\",\n",
    "    \"--labeled_ratio\", \"0.4\",\n",
    "    \"--aug\", \"random_bit_flip\",\n",
    "    \"--seed\", \"0\",\n",
    "    \"--lambda-u\", \"1\",\n",
    "    \"--T\", \"1\",\n",
    "    \"--wdecay\", \"0.0005\",\n",
    "    \"--nesterov\",\n",
    "    \"--lr\", \"0.03\",\n",
    "    \"--warmup\", \"0\",\n",
    "    \"--local_rank\", \"-1\"\n",
    "]\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Run FixMatch with Bit Flip Augmentation on MLP\")\n",
    "parser.add_argument(\"--bit_flip\", type=int, default=11, help=\"Number of bits to flip per sample\")\n",
    "parser.add_argument(\"--labeled_ratio\", type=float, default=0.4, help=\"Ratio of labeled data\")\n",
    "parser.add_argument(\"--aug\", type=str, default=\"random_bit_flip\", help=\"Augmentation function to use\")\n",
    "parser.add_argument(\"--seed\", type=int, default=0, help=\"Random seed for reproducibility\")\n",
    "parser.add_argument('--lambda-u', default=1, type=float, help='coefficient of unlabeled loss')\n",
    "parser.add_argument('--T', default=1, type=float, help='pseudo label temperature')\n",
    "parser.add_argument('--wdecay', default=5e-4, type=float, help='weight decay')\n",
    "parser.add_argument('--nesterov', action='store_true', default=True, help='use nesterov momentum')\n",
    "parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.03, type=float, help='initial learning rate')\n",
    "parser.add_argument('--warmup', default=0, type=float, help='warmup epochs (unlabeled data based)')\n",
    "\n",
    "# Simulate command-line arguments\n",
    "args = parser.parse_args(args_list)\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Load data\n",
    "path = \"/home/mhaque3/myDir/data/gen_apigraph_drebin/\"\n",
    "file_path = f\"{path}2012-01to2012-12_selected.npz\"\n",
    "data = np.load(file_path, allow_pickle=True)\n",
    "X, y = data['X_train'], data['y_train']\n",
    "y = np.array([0 if label == 0 else 1 for label in y])\n",
    "\n",
    "n_bit_flip = args.bit_flip\n",
    "labeled_ratio = args.labeled_ratio\n",
    "\n",
    "strategy = f\"fixmatch_wo_al_{args.aug}_{n_bit_flip}_lbr_{labeled_ratio}_seed_{args.seed}\"\n",
    "print(f\"Running {strategy}...\")\n",
    "print(f\"Using {n_bit_flip} bits to flip per sample. Labeled ratio: {labeled_ratio}, Seed: {args.seed}\")\n",
    "\n",
    "# Use the labeled_ratio argument here!\n",
    "X_labeled, y_labeled, X_unlabeled, y_unlabeled = split_labeled_unlabeled(X, y, labeled_ratio=labeled_ratio, random_state=args.seed)\n",
    "\n",
    "X_2012_labeled = torch.tensor(X_labeled, dtype=torch.float32).cuda()\n",
    "y_2012_labeled = torch.tensor(y_labeled, dtype=torch.long).cuda()\n",
    "X_2012_unlabeled = torch.tensor(X_unlabeled, dtype=torch.float32).cuda()\n",
    "y_2012_unlabeled = torch.tensor(y_unlabeled, dtype=torch.long).cuda()\n",
    "\n",
    "input_dim = X_2012_labeled.shape[1]\n",
    "num_classes = len(torch.unique(y_2012_labeled))\n",
    "\n",
    "\n",
    "\n",
    "model = Classifier(input_dim=input_dim, num_classes=num_classes).cuda()\n",
    "\n",
    "no_decay = ['bias', 'bn']\n",
    "grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(\n",
    "        nd in n for nd in no_decay)], 'weight_decay': args.wdecay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(\n",
    "        nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.SGD(grouped_parameters, lr=args.lr,\n",
    "                        momentum=0.9, nesterov=args.nesterov)\n",
    "\n",
    "train_fixmatch_drift_eval(\n",
    "    n_bit_flip,\n",
    "    model,\n",
    "    optimizer,\n",
    "    X_2012_labeled,\n",
    "    y_2012_labeled,\n",
    "    X_2012_unlabeled,\n",
    "    y_unlabeled,\n",
    "    args,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apkencoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
